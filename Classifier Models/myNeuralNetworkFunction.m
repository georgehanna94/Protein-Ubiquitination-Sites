function [y1] = myNeuralNetworkFunction(x1)
%MYNEURALNETWORKFUNCTION neural network simulation function.
%
% Generated by Neural Network Toolbox function genFunction, 02-Apr-2017 17:57:07.
%
% [y1] = myNeuralNetworkFunction(x1) takes these arguments:
%   x = 9xQ matrix, input #1
% and returns:
%   y = 2xQ matrix, output #1
% where Q is the number of samples.

%#ok<*RPMT0>

% ===== NEURAL NETWORK CONSTANTS =====

% Input 1
x1_step1_xoffset = [0.1;0.1;0.1;0.1;0.1;0.1;0.1;0.1;0.1];
x1_step1_gain = [2.22222222222222;2.22222222222222;2.22222222222222;2.22222222222222;2.22222222222222;2.22222222222222;2.22222222222222;2.22222222222222;2.22222222222222];
x1_step1_ymin = -1;

% Layer 1
b1 = [-1.7248118963517758;0.95113077703771876;-0.4193593044307628;0.80914993947525704;1.6910668557781701];
IW1_1 = [-0.022189437286616945 -1.0882053716602231 -0.94646127342284903 0.36547960012811392 -0.23130652087495807 -0.036154447383200505 0.049353598118620737 0.61344162456566109 0.024089143168566744;-0.54383123507882913 0.90964895826372005 -0.50811573450602832 -0.85156411178592228 -0.39166546719528883 -0.028576987993415896 -0.60295577675499468 0.043214437069429013 -0.058140089226372454;-1.0484438882267628 0.30096153422016048 -0.53805127375410478 0.54452511855223229 -0.70828296803189628 -0.32884634855544059 -0.056212653274556962 -0.46578166127227999 -0.83224471844222969;0.89118106858445512 0.44568985719620374 0.2496976128671583 0.82152800145913296 -0.40216184098613506 1.0304117394196699 0.15293990830515308 0.61329893953788606 -1.0663184343424641;0.26071370452564252 -0.37136928391973562 -0.33025429419405944 -0.65210898398045625 -0.45829378663668813 1.0172170266670246 0.45728723841241231 0.53744146481399369 -0.3727684199605103];

% Layer 2
b2 = [-0.18641484955840712;-0.21036953968215819];
LW2_1 = [0.081429667474305287 1.0068417896349766 0.71636874299568376 -1.561763609965851 0.45855411733013451;-1.0499445974935862 0.5042572704378453 -1.4437260989061311 1.5174025578604682 0.66812237795660889];

% ===== SIMULATION ========

% Dimensions
Q = size(x1,2); % samples

% Input 1
xp1 = mapminmax_apply(x1,x1_step1_gain,x1_step1_xoffset,x1_step1_ymin);

% Layer 1
a1 = tansig_apply(repmat(b1,1,Q) + IW1_1*xp1);

% Layer 2
a2 = softmax_apply(repmat(b2,1,Q) + LW2_1*a1);

% Output 1
y1 = a2;
end

% ===== MODULE FUNCTIONS ========

% Map Minimum and Maximum Input Processing Function
function y = mapminmax_apply(x,settings_gain,settings_xoffset,settings_ymin)
y = bsxfun(@minus,x,settings_xoffset);
y = bsxfun(@times,y,settings_gain);
y = bsxfun(@plus,y,settings_ymin);
end

% Competitive Soft Transfer Function
function a = softmax_apply(n)
nmax = max(n,[],1);
n = bsxfun(@minus,n,nmax);
numer = exp(n);
denom = sum(numer,1);
denom(denom == 0) = 1;
a = bsxfun(@rdivide,numer,denom);
end

% Sigmoid Symmetric Transfer Function
function a = tansig_apply(n)
a = 2 ./ (1 + exp(-2*n)) - 1;
end

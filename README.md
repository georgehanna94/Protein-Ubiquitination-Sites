# Protein-Ubiquitination-Sites
The goal of this project is to develop a pattern classification system using the given training dataset in order to accurately predict ubiquitination on a blind test dataset. 

## Dataset Description
The dataset (which is not included here due to size restrictions) is a collection of protein windows centered on lysine residues. 80% of the dataset is provided with its correctly  classified  classes:  the  positive  windows  correspond  to  the  sites  that  are  known  to  be ubiquitinated  and  the  negative  windows  are  assumed  to  be  ubiquitinated.  Among  the  dataset, only 30% of the data was labelled and each project team is allowed to request to label up to 1000 unlabelled  sites.  The  20%  of  the  total  dataset  is  withheld  as  a  blind  test  set  to  be  identified  as ubiquitination. Initial features for each site has been selected by the application of ProtDCal and reduced to 435 features. 

## High-Level Solution
We chose to use the multilayer  perceptron  as  a  pattern  classifier  and  genetic  algorithm  as  a  feature  selector. Multilayer  perceptron  is  a  logistic  regression  classifier  and  networks  with  one  or  more  layers between  input,  hidden and  output.  A  single  output  from  multilayers  of  inputs  is  formed according  to  its  input  weights  and  forwards  through  some  nonlinear  activation  function.  The power of multilayer perceptron is its learning ability from a training set and how to relate to the best to the output that an operator wants to predict. Genetic algorithm is derived from the natural selection, the process of biological evolution. At each step of the algorithm, it randomly selects individuals  from  the  present  generation  and  produces  the  next  generation.  Over  consecutive generations,  the ‘fittest’survivor among  individuals proceedsto  the  next  generation  and  the successful point in each competition provides a possible solution for solving a problem.

# Method
## 1) Data Pre-processing
Several  preprocessing  steps  needed  to  be  taken  prior  to  conducting  feature  selection  and  the training  of  the  classifier.  The networks trained through MATLAB’s Neural Network Toolbox generally  employ  sigmoid  transfer  functions  in  the  hidden  layers.  To  avoid  saturating  these functions during the training phase, MATLAB automatically normalizes the inputs of the dataset. Another  pre-processing  step  involved  the  handling  of  missing  values  (denoted  by -9999  in  the dataset  file).  To  ensure  the  proper  classification  of  samples  which  include  missing  values amongst some of their features, the missing values were replaced by the column medians. Outlier detection  was  another  important  pre-processing  step,  which  would  ultimately  enhance  the classifier’s robustness towards noise. To accomplish this, the mean absolute deviation of values within  each  feature  column  were  calculated.  Values  that  were  more  than  10  deviations  away from  the  median  were  replaced  by  the  median.  This  process  was  used  as  it  is  less  error  prone than standard deviations from the mean.The   final   pre-processing   step   involved   testing   different   techniques   for   mitigating   class imbalance. Four methods were examined and compared iteratively during the training and testing of  the  classifiers.  The  first  technique  was  the  synthetic  minority  oversampling  technique (SMOTE);  it  was  used  on  70%  of  the  labelled  data  to  equalize  the  number of  samples  in  each class. Adaptive SMOTE is second technique which was used; it samples the minority class near the  boundary  between  both  classes.  The  two  final  techniques  explored  include  penalizingthe algorithm for misclassifying positive class samples and undersampling of the majority class. The four  methods  discussed  above  were  to  be  evaluated during  the  testing  of  the  classifier. 

## 2) Feature Selection
For the feature  selection  method,  we identified two  methods  to  increase  performance  of  our classifier(in  this  case,  performance  is  the  optimal  precision/recall  point).  First,  we  selected CfsSubsetEval  in  Weka  to  perform  feature  selection.  This  evaluator  considers  the  individual predictive ability of each feature and  evaluates a  possible subset of attributes. The subset of features are correlated with the class. After a time-consuming and computationally heavy greedy search, we generated 60 selected features. Greedy algorithm provides an optimal choice at each stage,  the  chosen  solution  contributes  to  a  solution  and  iterates.  Based  on  the resulting features from greedy search, we implemented a genetic search feature selection method in Weka to boost the  accuracy  of  the  selected  features.The  genetic  algorithm  resultedin a  similar  number  of features to the  greedy  search.However,  the individual  elements  in  genetic  algorithm  are evaluated from high performance ‘offspring’ from their parents, as a result, this leads to an improvement of the dataset and their features to the given goal.
